---
title: "Extract from pdf - cruise ship schedule"
author: "Martin Monkman"
date: "24/03/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Victoria Cruise schedule: https://gvha.ca/cruise/cruise-schedule/ 

Webpage can't sort or filter

The PDF file has more data (where the ship came from and its next port)


## Extract the data from the PDF file

pdftools
https://ropensci.org/blog/2016/03/01/pdftools-and-jeroen/

tabulizer

```{r}

library(pdftools)

```




```{r}

#download.file("https://gvha.ca/wp-content/uploads/2019/04/2019-cruise-schedule-19-04-12.pdf",
#                        "2019_cruise_schedule.pdf")

vc_text <- pdf_text("2019-cruise-schedule-19-04-12.pdf")

```


This creates an object `vc_text`, which is a list of 4 items--one for each page in the original PDF file. Let's look at page one:

```{r}

vc_text[1]

```


Pull it apart using regex and the **stringr** package

https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html

Since we are going to be looking for the string "\r\n" -- a combination of carriage return "\r" and line return "\n" -- we need to puzzle through how R uses regular expressions; https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html

Rather than look for the explicit text strings, we can use the function `regex("\u000D\u000A")` which will find the combination of both the carriage and line returns.

```{r}

library(stringr)
library(tidyr)


str_detect(vc_text, regex("\u000D\u000A"))

```


### some string and vector utilities

https://stackoverflow.com/questions/46583363/string-splitting-a-dataframe-with-a-vector-as-the-pattern-in-r

https://stringr.tidyverse.org/reference/str_split.html 

https://stackoverflow.com/questions/652136/how-can-i-remove-an-element-from-a-list



...for more on how to parse these, see the chapter [Vectors](https://r4ds.had.co.nz/vectors.html) in the book [_R for Data Science_](https://r4ds.had.co.nz/) by Garrett Grolemund and Hadley Wickham.


***

## Tidying the data

Our goal is a tidy data frame (or [tibble](https://tibble.tidyverse.org/)) with one observation (ship visit) per row, and the various characteristics of that visit (date, origin, etc.) in each column. At the moment, we've got two untidy aspects to deal with:

- There is superfluous page header and footer material on each of the 4 PDF pages; this needs to be stripped out.

- Each row is currently one list per line; these need to be separated into each of the variables.


First, create a split version of the full file, finding the carriage and line returns, and using that to split the page using `stringr::str_split()`, which results in one list per line. Our new object `vc_text_split` is a list of lists...



```{r}

vc_text_split <- str_split(vc_text, "\u000D\u000A")

```

Now we have a list `vc_text_split` that has 4 items, each with multiple lists within it--a list of lists. 



### Remove superfluous rows

#### Headers

We will start by looking at the first 6 rows of pages 1, 2, and 10 to compare their structure. First, page 1:

```{r}

vc_text_split[[1]][1:6]   

```

Based on the first page, the first 2 rows are the header, and row 3 is the variable names. Let's see if that holds true for page two.

We can use a `for` loop to see pages 2 through 4:

```{r}


for (i in 1:4){
  print(glue::glue("----------", "page ", i))
  print(
  vc_text_split[[i]][1:5]
  )
}



```


A nice neat pattern. The variable names appear at the top of page one, and are omitted the rest of the way through the document.



#### Footers

What about the bottom of the pages? Here's a quick glance at the bottom of all the pages, using the `tail()` function) shows this:

```{r}

for (i in 1:4){
  print(glue::glue("----------", "page ", i))
  print(
  tail(vc_text_split[[i]])
  )
}

```

Each page has a blank row at the end, which will have to be deleted.


### Step 1


So let's make a clean page one. To do this, we can write a function that finds the last row (using `length`) and then omit it, using the `-` sign in front of our index.

Here's the function, and a test of the first page.

```{r}

last_x_line_fun <- function(page_num){
  last_x_line <- length(page_num)
  page_num[-(last_x_line)]
}

page_num = vc_text_split[[1]]

tail(last_x_line_fun(page_num))

```

Now, a loop of all four pages, creating a new object for each page:

```{r}

for (p in 1:4){
  assign(glue::glue("page", p), 
         last_x_line_fun(vc_text_split[[p]]))
}

```




### References

https://medium.com/@CharlesBordet/how-to-extract-and-clean-data-from-pdf-files-in-r-da11964e252e

https://www.r-bloggers.com/how-to-extract-data-from-a-pdf-file-with-r/


tabulizer -- R package to pull tables from pdfs (seems a bit finicky)
https://github.com/ropensci/tabulizer

